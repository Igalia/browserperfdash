WebBrowsers Performand Dashboard
---------------------------------

This application will receive performance data from several bots that
will be running continously different performance tests with different
browsers and versions of this browsers.


The application should have an administrative panel where the admin of
the tool can create a new entry for the bots with different fields.


The admin interface would have this functionalities / menus:


* Manage bots:

 * Add new bot

	The data that should be required for the bot will be:
	 * Bot id: The identifier for this bot, can be a number or a string. It must be unique. The application can generate it automatically.
	 * Bot password: This will allow the bot to authenticate itself correctly when sending the data.
	 * Bot name: Free text field
	 * CPU Architecture: a drop-down menu that allows to choose from the available and enabled architectures.
	 * CPU details: the model, number of cores or speed (free text field)
	 * GPU type: a drop-down menu that allows to choose from the available and enabled GPU types.
	 * GPU details: the model or the version of the grahpics drivers (free text field)
	 * Platform: a drop-down menu that allows to choose from the available and enabled platforms.
	 * Platform details: Debian 8, Ubuntu 16.04, Fedora 25 with wayland ... (free text field)

 * Modify bot:
     * Allows to modify any of the fields (except the bot_id) for a given bot.

 * Enable bot:
     * Drop-down menu: the admin can enable back any of the bots disabled.
       If the bot to be enabled contains a CPU architecture, GPU type or
       Platform currently disabled an error would be generated.

 * Disable bot:
      * Drop-down menu: the admin can disable any of the bots available and enabled.


* Manage CPU architectures:

 * Add new CPU Architecture:
		* cpu_id: The identifier for the architecture. Must be unique. Can be autogenerated.
		* cpu_name: x86, x64, ARMv7, ARM64, MIPS, ARMv7-Thumb2 (free text field).
		            Must be unique also, if when adding a new architecture there is already
		            one with the same name (even if is disabled) an error should be generated.


 * Modify CPU Architecture:
     * Allows to change the name for a CPU architecture. The name should be checked also
       to be unique like when creating a new one.

 * Disable CPU Architecture:
    * drop-down to select from the currently availables.
      If any of the bots contains the architecture to be disabled, a warning
      would be generated with the lists of bots that contain that.
      Then the admin would be asked twice to continue. If the admins continues
      disabling this architecture, then all the bots containing it will be
      also automatically disabled.


 * Enable  CPU Architecture:
    * drop-down to select from the currently ones disabled.


* Manage GPU types:

 * Add new GPU:
		* cpu_id: The identifier for the GPU. Must be unique. Can be autogenerated.
		* cpu_name: NVIDIA, Intel, AMD (free text field)
		            Must be unique also, if when adding a new GPU there is already
		            one with the same name (even if is disabled) an error should be generated.

 * Modify GPU:
     * Allows to change the name for a GPU. The name should be checked also
       to be unique like when creating a new one.

 * Disable GPU:
    * drop-down to select from the currently availables.
      If any of the bots contains the GPU type to be disabled, a warning
      would be generated with the lists of bots that contain that.
      Then the admin would be asked twice to continue. If the admins continues
      disabling this architecture, then all the bots containing it will be
      also automatically disabled.

 * Enable  GPU:
    * drop-down to select from the currently ones disabled.



* Manage Platforms:

 * Add new GPU:
		* platform_id: The identifier for the Platform. Must be unique. Can be autogenerated.
		* platform_name: linux-x11, linux-wayland, macos-sierra, macos-mavericks, .. (free text field)
		            Must be unique also, if when adding a new platform there is already
		            one with the same name (even if is disabled) an error should be generated.

 * Modify Platform:
     * Allows to change the name for a Platform. The name should be checked also
       to be unique like when creating a new one.

 * Disable Platform:
    * drop-down to select from the currently availables.
      If any of the bots contains the Platform to be disabled, a warning
      would be generated with the lists of bots that contain that.
      Then the admin would be asked twice to continue. If the admins continues
      disabling this architecture, then all the bots containing it will be
      also automatically disabled.

 * Enable  Platform:
    * drop-down to select from the currently ones disabled.


* Manage browsers:

  *  Add new browser:
   - browser_id: string identifying the browser. Must be unique.
                 The string can be choosen by the admin. When creating a new one
                 the application would check that is unique or would return an error.
   - browser_name: Free text field with the name of the browser.

  * Enable/Disable a browser:
   - Drop-down menu: the admin can disable any of the browsers available or enable back any
     of the ones disabled.


* Manage tests:

  * Add new test:
   - test_id:    string identifiyng the browser. Must be unique.
                 The string can be choosen by the admin. When creating a new one
                 the application would check that is unique or would return an error.
   - test_description:  Free text field with the name or details of the test.
   - test_url:          An URL to the test (optional)


  * Enable/Disable a test:
   - Drop-down menu: the admin can disable any of the browsers available or enable back any
     of the ones disabled.


* Show bot report logs:

   * Will show a simple logfile containing debug information about the recent reports received.
     Example:

   This log can be a simply text file generated by the application and that is shown on the admin
   interface when requested.

   2016/11/17T19:02:00 Correctly: received data for bot $bot_id browser $browser_name test $test_id
   2016/11/13T10:13:00 ERROR: bot $bot_id tried to send data for browser $unknown_browser_name which is unknown
   2016/11/11T13:12:00 ERROR: bot $bot_id tried to send data for test $test_browser_name which is unknown
   2016/11/13T19:12:00 ERROR: parsing JSON file from $bot_id ...


* Manage metric units:

  * Add new metric.
   - metric_name:    		string identifiyng the metric unit. Must be unique.
   - metric_unit: 		a short string identifiying the unit
   - metric_description: 	Free text field
   - metric_better_is:   	a drop-down to select between: "up" and "down".
   * Add metric-prefix: 	here one can input a symbol and a number.
   * Modify metric-prefx
   * Delete metric-prefx

  * Modify metric.


For example, for the metric BrowserMemoryPeak I would input:


   - metric_name: 	 	BrowserMemoryPeak
   - metric_unit:	 	bytes
   - metric_description:	Peak memory used by the browser during the test.
   - metric_better_is:   	down
   * Add metric-prefix:
			      KB - 1024
			      MB - 1048576
			      GB - 1073741824


And for the metric "Score" I would input:

   - metric_name: 	 	Score
   - metric_unit:	 	points
   - metric_description:	Performance points achieved on the benchmark.
   - metric_better_is:   	up
   * Add metric-prefix:		<nothing>


And for the metric "Time" i would input:


   - metric_name: 	 	Time
   - metric_unit:	 	seconds
   - metric_description:	Time it took to finish the benchmark
   - metric_better_is:   	down
   * Add metric-prefix:
			      minutes - 60
			      hours   - 3600



Receiving data from the bots
-----------------------------

The application would expose a resource, for example https://benchmark.igalia.com/bot-report
where the bot would do a POST request to send the data of the benchmarks.

In this POST request the bot would include the following information as post parameters:

* bot_id
* bot_password
* browser_id: string identifying the browser.
* browser_version: a number or string.
* test_id: short name identifying the test.
* test_version: a number or string.
* test_data -> the json_file with the data itself


When receiving this data, the bot_id and bot_password will be checked.
If any of this fails the application would return an http error code
403 forbidden and ignore the request.

If the bot_id and bot_password matches one of the bots the application
knows about, then it will generate a log entry on the logfile and
process the data received as follows:

 - if the browser_name don't matches one of the browsers the application
   already knows about, it will return and error and ignore the data.

 - if the test_id don't matches one of the tests the applicacion
   already knows about, it will return and error and ignore the data

 - if the metric_name don't matches one of the metric units the applicacion
   already knows about, it will return and error and ignore the data

 - If all the checks above pass, then it will process the data and store
   it on the database.


Processing the data and storing it on the database:

Each result stored in the database should be stored in a way that matching
it against the following values is possible and efficient:

bot_id, browser_name, browser_version, test_name, test_version, timestamp

The timestamp is the date (unix epoch) when the result was received by the server.



About the json files
--------------------

If you check the JSON data files generated by the bots you will see the
following format:

The value for a test is as follows. There are two possibilities:

```pre
{ ${NameOfTest}: {metrics: { ${NameOfMetric} : { current : [ list of values ] } } } }

{ ${NameOfTest}: {metrics: { ${NameOfMetric} : [ AggregationAlgorithm ] }}}
```

The first option indicates that this specific test has real values for that metric
produced from the browser while running it.

The second option indicates that the values for this specific test are calculated
by aggregating the values of all the subtests it contains.

A test can also contain subtests. Its mandatory that a test with aggreated values
contain subtests.


This is explained better with an example:



```json
{"SpeedometerExample": {"metrics": {"Score": {"current": [142, 141, 143, 141, 143] },
                         "Time": ["Total"]},
                         "tests": {"AngularJS-TodoMVC": {"metrics": {"Time": ["Total"]},
                                                        "tests": {"Adding100Items": {"metrics": {"Time": ["Total"]},
                                                                                   "tests": {"Async": {"metrics": {"Time": {"current": [9, 11, 10, 12.25, 14 ]}}},
                                                                                              "Sync": {"metrics": {"Time": {"current": [207, 198, 214, 203.8, 210]}}}}},
                                                                  "Adding200Items": {"metrics": {"Time": ["Total"]},
                                                                                   "tests": {"Async": {"metrics": {"Time": {"current": [19, 21, 20, 32.25, 44 ]}}},
                                                                                              "Sync": {"metrics": {"Time": {"current": [407, 398, 414, 403.8, 523]}}}}}}}}}}
```



Here we have:

 * Main test suite with name SpeedometerExample with two metrics: Score and Time.
   * The metric "Score" for SpeedometerExample is calculated directly: we take the list of values in current and we calculate the mean (arithmetic mean)
   * The metric "Time" for SpeedometerExample is an aggregated value that is calculated using the "Total" agregation algorithm with the values of the subtests.

   * Subtest AngularJS-TodoMV has one metric Time of aggregated value Total with the values of the subtests

      * Subtests Adding100Items has one metric Time of aggregated value Total with the values of the subtests

	 * Subtests Async has one metric Time with current values: we calculate the mean of the values.
	 * Subtests Sync has one metric Time with current values: we calculate the mean of the values.

      * Subtests Adding200Items has one metric Time of aggregated value Total with the values of the subtests

	 * Subtests Async has one metric Time with current values: we calculate the mean of the values.
	 * Subtests Sync has one metric Time with current values: we calculate the mean of the values.


Check the attached file [sample-files/speedometerexample.result](sample-files/speedometerexample.result)  that contains
the above example, and run the example script ```read-json-results``` from this directory (```docs```)

* To generate a python dictionary with all the values calculated and print it:
```pre
./read-json-results -print-results-dict sample-files/speedometerexample.result
```

* To print the results on plain text:
```pre
./read-json-results  -print-results-text sample-files/speedometerexample.result
SpeedometerExample:Score: 142.000pt stdev=0.7%
                  :Time:Total: 674.220ms stdev=9.8%
                  AngularJS-TodoMVC:Time:Total: 674.220ms stdev=9.8%
                                   Adding100Items:Time:Total: 217.810ms stdev=2.9%
                                                 Async:Time: 11.250ms stdev=17.4%
                                                 Sync:Time: 206.560ms stdev=2.9%
                                   Adding200Items:Time:Total: 456.410ms stdev=13.6%
                                                 Async:Time: 27.250ms stdev=39.6%
                                                 Sync:Time: 429.160ms stdev=12.3%

```
* To print the results on a format like what is inserted into this application database use :
```pre
./read-json-results -print-results-db sample-files/speedometerexample.result
Name=SpeedometerExample      Metric=Score\None      Unit=pt      Value=142.0      Stdev=0.00704225352113
Name=SpeedometerExample      Metric=Time\Total      Unit=ms      Value=674.22      Stdev=0.098284410446
Name=SpeedometerExample\AngularJS-TodoMVC      Metric=Time\Total      Unit=ms      Value=674.22      Stdev=0.098284410446
Name=SpeedometerExample\AngularJS-TodoMVC\Adding100Items      Metric=Time\Total      Unit=ms      Value=217.81      Stdev=0.0290934151339
Name=SpeedometerExample\AngularJS-TodoMVC\Adding100Items\Async      Metric=Time\None      Unit=ms      Value=11.25      Stdev=0.173561103909
Name=SpeedometerExample\AngularJS-TodoMVC\Adding100Items\Sync      Metric=Time\None      Unit=ms      Value=206.56      Stdev=0.0294749686776
Name=SpeedometerExample\AngularJS-TodoMVC\Adding200Items      Metric=Time\Total      Unit=ms      Value=456.41      Stdev=0.136262489719
Name=SpeedometerExample\AngularJS-TodoMVC\Adding200Items\Async      Metric=Time\None      Unit=ms      Value=27.25      Stdev=0.395773479085
Name=SpeedometerExample\AngularJS-TodoMVC\Adding200Items\Sync      Metric=Time\None      Unit=ms      Value=429.16      Stdev=0.122973388375
```

On the ```sample-files``` diretory there are more example of json files that were generated on the bots.

You can use the ```read-json-results``` script in this directory to see how to parse and process them.


Frontend
--------

## Improvements / Regressions tables.

We will show a table that shows the biggests regressions or improvements on the last days.

The user should be able to select the data to show:

 * Number of days: how many days back should we fetch the data to compare.
 * Browsers: filter the data by a specific browser name or show from all the browsers aggregated
 * CPUs: filter by a specific CPU or not
 * GPUs: filter by a specific GPU or not
 * Platforms: filter by a specific platform or not
 * Bot: filter the data to show only the one coming from one of the bots or not.


For the raw performance number we will ignore the aggregated values. That is: those that were calculated by aggregating values from subtests.

The input data for generating this table will filter any aggregated value.

Something similar to this: https://chromeperf.appspot.com but different as we will support data from multiple browsers, platforms, etc.


## Performance graphs

Here the user should be able to plot a graph for a given test.

The user can select how many data together wants to plot.

Some examples:

 * Plot a graph for Time:SpeedometerExample/AngularJS-TodoMVC/Adding100Items/Async with 1 line for chrome browser on bot raspberry-pi3.
 * Plot a graph for Time:SpeedometerExample/AngularJS-TodoMVC/Adding100Items/Async with n lines for chrome browser on all the n bots that have results for this test for chrome.
 * Plot a graph for Time:SpeedometerExample/AngularJS-TodoMVC/Adding100Items/Async with n+z lines for the the chrome browsers above plus all the z firex ones.
 * Plot a graph for Score:SpeedometerExample with data from browser firefox on GPU Intel
 * Plot a graph for Time-aggregated:SpeedometerExample with data from browser minibrowser-gtk on platform linux-x11.


About the data on the graphs:
* The x-axes of the graph will be the time (date on which the result was produced)
* The y-axes of the graph will be the values for the metric.


Notes:

* For the performance graphs we allow to show the aggregated values.
  * We will plot this values differently (perhaps using ... dots instead of ___ lines) so its easy to identify that is an aggregated value.


Examples of browser performance related graphs:

* https://arewefastyet.com/
* https://chromeperf.appspot.com/report
* https://perf.webkit.org/



Format of the JSON files and what to store in the database
-----------------------------------------------------------

This are the design considerations that were taken into account when
designing the format for storing the data on the database.

We want the application to be fast. That it can draw the graphs quickly.

And we want to support drawing graphs of aggregated values. So if we have
to calculate this values on the fly we are adding extra calculations that
are not really needed (over and over).

So I think is a better idea to store the values already pre-calculated on
the database.

We need to know if a value is an aggregated or not. So I think the best
idea would be to store all the values on the database with a field indicating
if the value is a real one ('current' on the JSON files) or is an aggregated
Indicating the type.

I do this with the "-print-results-dict" output of ./read-json-results.
By using the keyword "None" for real values, and indicating the aggregation
algorithm for the others.

```pre
./read-json-results -print-results-dict sample-files/speedometerexample.result

{u'SpeedometerExample': {'metrics': {u'Score': {None: {'mean_value': 142.0,
                                                       'raw_values': [142.0,
                                                                      141.0,
                                                                      143.0,
                                                                      141.0,
                                                                      143.0],
                                                       'stdev': 0.007042253521126761,
                                                       'unit': 'pt'}},
                                     u'Time': {u'Total': {'mean_value': 674.22,
                                                          'raw_values': [642.0,
			[....]
```

Here we see:

 * The value 142.0 of Score:SpeedometerExample is a real-one ('None' key)
 * The value 674.22 of Time:SpeedometerExample is an aggregated one (using the aggregation algorithm "Total")

The algorithm for aggregation are 3:

```pre
    aggregators = {
        'Total': (lambda values: sum(values)),
        'Arithmetic': (lambda values: sum(values) / len(values)),
        'Geometric': (lambda values: math.exp(sum(map(math.log, values)) / len(values))),
    }
```

So I think that we can store this on the DB, have an aggregation field that
indicates how the value was generated. For real values we can use a keyword
line "current" or "None".


We have two options:

 * Receive on the server the JSON files as they are beeing generated now (see the examples),
   and upon receiving them calculate all the aggregated values, the mean_values, stdev, etc
   and store on the DB the values calculated

 * Do this step on the bots, and send to the server the JSON file already calculated.
   Something similar to what the command -print-results-dict-json already outputs.

Final implementation:

  * On the ```benchmark_results.py``` file the method ```_generate_db_entries``` is used to
format the data from the json files to the database format. You can see it on the screen
by executing the script ```./read-json-results``` from this directory (```docs```) as follows:

```pre
./read-json-results -print-results-db sample-files/speedometerexample.result
Name=SpeedometerExample      Metric=Score\None      Unit=pt      Value=142.0      Stdev=0.00704225352113
Name=SpeedometerExample      Metric=Time\Total      Unit=ms      Value=674.22      Stdev=0.098284410446
Name=SpeedometerExample\AngularJS-TodoMVC      Metric=Time\Total      Unit=ms      Value=674.22      Stdev=0.098284410446
Name=SpeedometerExample\AngularJS-TodoMVC\Adding100Items      Metric=Time\Total      Unit=ms      Value=217.81      Stdev=0.0290934151339
Name=SpeedometerExample\AngularJS-TodoMVC\Adding100Items\Async      Metric=Time\None      Unit=ms      Value=11.25      Stdev=0.173561103909
Name=SpeedometerExample\AngularJS-TodoMVC\Adding100Items\Sync      Metric=Time\None      Unit=ms      Value=206.56      Stdev=0.0294749686776
Name=SpeedometerExample\AngularJS-TodoMVC\Adding200Items      Metric=Time\Total      Unit=ms      Value=456.41      Stdev=0.136262489719
Name=SpeedometerExample\AngularJS-TodoMVC\Adding200Items\Async      Metric=Time\None      Unit=ms      Value=27.25      Stdev=0.395773479085
Name=SpeedometerExample\AngularJS-TodoMVC\Adding200Items\Sync      Metric=Time\None      Unit=ms      Value=429.16      Stdev=0.122973388375
```
